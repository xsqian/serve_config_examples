{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-03-29 12:03:32</td></tr>\n",
       "<tr><td>Running for: </td><td>00:39:57.91        </td></tr>\n",
       "<tr><td>Memory:      </td><td>33.7/64.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 3.0/12 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     loss</th><th style=\"text-align: right;\">  epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_7458a_00000</td><td>TERMINATED</td><td>127.0.0.1:75003</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">          2393.8</td><td style=\"text-align: right;\">0.0493913</td><td style=\"text-align: right;\">      9</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(TorchTrainer pid=75003)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=75003)\u001b[0m - (ip=127.0.0.1, pid=75027) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=75003)\u001b[0m - (ip=127.0.0.1, pid=75028) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m [W ProcessGroupGloo.cpp:751] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m Moving model to device: cpu\n",
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /var/folders/0n/0gc794z54nv7z0mdwxbh998h0000gp/T/data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26421880 [00:00<?, ?it/s]\n",
      "  0%|          | 32768/26421880 [00:00<02:09, 203506.24it/s]\n",
      "  7%|▋         | 1769472/26421880 [00:01<00:09, 2625544.46it/s]\n",
      " 44%|████▍     | 11665408/26421880 [00:03<00:03, 4759212.57it/s]\n",
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m [W ProcessGroupGloo.cpp:751] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      " 92%|█████████▏| 24379392/26421880 [00:04<00:00, 7713758.46it/s]\n",
      " 96%|█████████▌| 25362432/26421880 [00:04<00:00, 5374700.63it/s]\n",
      "100%|██████████| 26421880/26421880 [00:05<00:00, 5237262.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m Extracting /var/folders/0n/0gc794z54nv7z0mdwxbh998h0000gp/T/data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /var/folders/0n/0gc794z54nv7z0mdwxbh998h0000gp/T/data/FashionMNIST/raw\n",
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /var/folders/0n/0gc794z54nv7z0mdwxbh998h0000gp/T/data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26421880 [00:00<?, ?it/s]\n",
      " 83%|████████▎ | 21987328/26421880 [00:05<00:00, 7749730.09it/s]\u001b[32m [repeated 60x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /var/folders/0n/0gc794z54nv7z0mdwxbh998h0000gp/T/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29515 [00:00<?, ?it/s]\n",
      "100%|██████████| 4422102/4422102 [00:01<00:00, 2516749.04it/s]\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      " 83%|████████▎ | 3670016/4422102 [00:02<00:00, 2477183.58it/s]\u001b[32m [repeated 27x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m Using downloaded and verified file: /var/folders/0n/0gc794z54nv7z0mdwxbh998h0000gp/T/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m Extracting /var/folders/0n/0gc794z54nv7z0mdwxbh998h0000gp/T/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /var/folders/0n/0gc794z54nv7z0mdwxbh998h0000gp/T/data/FashionMNIST/raw\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m \u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/Xingsheng_Qian/ray_results/TorchTrainer_2024-03-29_11-23-32/TorchTrainer_7458a_00000_0_2024-03-29_11-23-34/checkpoint_000000)\n",
      "100%|██████████| 5148/5148 [00:00<00:00, 13761808.15it/s]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "100%|██████████| 4422102/4422102 [00:02<00:00, 1753565.90it/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m {'loss': 0.20273448526859283, 'epoch': 0}\n",
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /var/folders/0n/0gc794z54nv7z0mdwxbh998h0000gp/T/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/Xingsheng_Qian/ray_results/TorchTrainer_2024-03-29_11-23-32/TorchTrainer_7458a_00000_0_2024-03-29_11-23-34/checkpoint_000001)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m {'loss': 0.17315298318862915, 'epoch': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/Xingsheng_Qian/ray_results/TorchTrainer_2024-03-29_11-23-32/TorchTrainer_7458a_00000_0_2024-03-29_11-23-34/checkpoint_000002)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m {'loss': 0.3881944715976715, 'epoch': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/Xingsheng_Qian/ray_results/TorchTrainer_2024-03-29_11-23-32/TorchTrainer_7458a_00000_0_2024-03-29_11-23-34/checkpoint_000003)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m {'loss': 0.19966208934783936, 'epoch': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/Xingsheng_Qian/ray_results/TorchTrainer_2024-03-29_11-23-32/TorchTrainer_7458a_00000_0_2024-03-29_11-23-34/checkpoint_000004)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m {'loss': 0.25064101815223694, 'epoch': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/Xingsheng_Qian/ray_results/TorchTrainer_2024-03-29_11-23-32/TorchTrainer_7458a_00000_0_2024-03-29_11-23-34/checkpoint_000005)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m {'loss': 0.27079126238822937, 'epoch': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/Xingsheng_Qian/ray_results/TorchTrainer_2024-03-29_11-23-32/TorchTrainer_7458a_00000_0_2024-03-29_11-23-34/checkpoint_000006)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m {'loss': 0.15984489023685455, 'epoch': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/Xingsheng_Qian/ray_results/TorchTrainer_2024-03-29_11-23-32/TorchTrainer_7458a_00000_0_2024-03-29_11-23-34/checkpoint_000007)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m {'loss': 0.178643599152565, 'epoch': 7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/Xingsheng_Qian/ray_results/TorchTrainer_2024-03-29_11-23-32/TorchTrainer_7458a_00000_0_2024-03-29_11-23-34/checkpoint_000008)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m {'loss': 0.13146929442882538, 'epoch': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75028)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/Users/Xingsheng_Qian/ray_results/TorchTrainer_2024-03-29_11-23-32/TorchTrainer_7458a_00000_0_2024-03-29_11-23-34/checkpoint_000009)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=75027)\u001b[0m {'loss': 0.04939134791493416, 'epoch': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 12:03:32,897\tINFO tune.py:1042 -- Total run time: 2397.93 seconds (2397.91 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "import ray.train.torch\n",
    "\n",
    "def train_func():\n",
    "    # Model, Loss, Optimizer\n",
    "    model = resnet18(num_classes=10)\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "    )\n",
    "    # [1] Prepare model.\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "    # model.to(\"cuda\")  # This is done by `prepare_model`\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Data\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    data_dir = os.path.join(tempfile.gettempdir(), \"data\")\n",
    "    train_data = FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "    # [2] Prepare dataloader.\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_loader)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(10):\n",
    "        if ray.train.get_context().get_world_size() > 1:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            # This is done by `prepare_data_loader`!\n",
    "            # images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # [3] Report metrics and checkpoint.\n",
    "        metrics = {\"loss\": loss.item(), \"epoch\": epoch}\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            torch.save(\n",
    "                model.module.state_dict(),\n",
    "                os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            ray.train.report(\n",
    "                metrics,\n",
    "                checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "            )\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "            print(metrics)\n",
    "\n",
    "# [4] Configure scaling and resource requirements.\n",
    "scaling_config = ray.train.ScalingConfig(num_workers=2, use_gpu=False)\n",
    "\n",
    "# [5] Launch distributed training job.\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=scaling_config,\n",
    "    # [5a] If running in a multi-node cluster, this is where you\n",
    "    # should configure the run's persistent storage that is accessible\n",
    "    # across all worker nodes.\n",
    "    # run_config=ray.train.RunConfig(storage_path=\"s3://...\"),\n",
    ")\n",
    "result = trainer.fit()\n",
    "\n",
    "# [6] Load the trained model.\n",
    "with result.checkpoint.as_directory() as checkpoint_dir:\n",
    "    model_state_dict = torch.load(os.path.join(checkpoint_dir, \"model.pt\"))\n",
    "    model = resnet18(num_classes=10)\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "    )\n",
    "    model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scaling_config \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mScalingConfig(num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, use_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ray' is not defined"
     ]
    }
   ],
   "source": [
    "scaling_config = ray.train.ScalingConfig(num_workers=2, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
